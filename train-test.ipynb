{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # first gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:21.734348Z",
     "start_time": "2024-02-06T08:48:21.729111Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:27.601417Z",
     "iopub.status.busy": "2024-02-06T08:35:27.600854Z",
     "iopub.status.idle": "2024-02-06T08:35:27.614489Z",
     "shell.execute_reply": "2024-02-06T08:35:27.613523Z"
    }
   },
   "outputs": [],
   "source": [
    "training = True\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:22.005580Z",
     "start_time": "2024-02-06T08:48:21.982746Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:27.619483Z",
     "iopub.status.busy": "2024-02-06T08:35:27.619091Z",
     "iopub.status.idle": "2024-02-06T08:35:29.958256Z",
     "shell.execute_reply": "2024-02-06T08:35:29.957233Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import random\n",
    "import shutil\n",
    "from tabulate import tabulate\n",
    "from itables import init_notebook_mode\n",
    "from itables import show\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from hist_metrics import *\n",
    "\n",
    "path_1 = 'results/'\n",
    "path_2 = 'logs/'\n",
    "eval_results_path = 'eval_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:22.100928Z",
     "start_time": "2024-02-06T08:48:22.057799Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:29.963505Z",
     "iopub.status.busy": "2024-02-06T08:35:29.962995Z",
     "iopub.status.idle": "2024-02-06T08:35:30.011103Z",
     "shell.execute_reply": "2024-02-06T08:35:30.009952Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_and_remove_dirs():\n",
    "    for i in range(5):\n",
    "        if os.path.exists(path_1):\n",
    "            shutil.rmtree(path_1)\n",
    "\n",
    "        if os.path.exists(path_2):\n",
    "            shutil.rmtree(path_2)\n",
    "        \n",
    "        if os.path.exists(eval_results_path):\n",
    "            shutil.rmtree(eval_results_path)\n",
    "            \n",
    "    if not os.path.exists(path_1):\n",
    "        os.makedirs(path_1)\n",
    "        os.makedirs(path_2)\n",
    "        os.makedirs(eval_results_path)\n",
    "        \n",
    "\n",
    "if (training):\n",
    "    create_and_remove_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:24.262004Z",
     "start_time": "2024-02-06T08:48:22.116254Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:30.017198Z",
     "iopub.status.busy": "2024-02-06T08:35:30.016579Z",
     "iopub.status.idle": "2024-02-06T08:35:30.658086Z",
     "shell.execute_reply": "2024-02-06T08:35:30.657127Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    #tf.config.experimental.set_visible_devices(gpus[g_n], 'GPU')\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "\n",
    "print(tf. __version__)\n",
    "# print(tf.keras. __version__)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:24.274407Z",
     "start_time": "2024-02-06T08:48:24.266726Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:30.661364Z",
     "iopub.status.busy": "2024-02-06T08:35:30.661144Z",
     "iopub.status.idle": "2024-02-06T08:35:30.665577Z",
     "shell.execute_reply": "2024-02-06T08:35:30.664756Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 13334\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "cp.random.seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:24.557271Z",
     "start_time": "2024-02-06T08:48:24.276823Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:30.669320Z",
     "iopub.status.busy": "2024-02-06T08:35:30.668987Z",
     "iopub.status.idle": "2024-02-06T08:35:30.678012Z",
     "shell.execute_reply": "2024-02-06T08:35:30.676949Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from PIL import Image, ImageStat\n",
    "import math\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "tnp.experimental_enable_numpy_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:24.772236Z",
     "start_time": "2024-02-06T08:48:24.561346Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:30.682907Z",
     "iopub.status.busy": "2024-02-06T08:35:30.682570Z",
     "iopub.status.idle": "2024-02-06T08:35:30.880719Z",
     "shell.execute_reply": "2024-02-06T08:35:30.879589Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "batch_size= 8\n",
    "\n",
    "imgz_size = 256\n",
    "\n",
    "\n",
    "train_image_dir = \"\"\n",
    "train_mask_dir = \"\"\n",
    "# val_image_dir = \"\"\n",
    "# val_mask_dir = \"\"\n",
    "\n",
    "import albumentations as A\n",
    "aug = A.Compose([\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def binarize_mask(mask):\n",
    "    mask = np.where(mask>0, 1, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image = aug(image=image)['image']\n",
    "    return image\n",
    "\n",
    "\n",
    "img_data_gen_args = dict( preprocessing_function = normalize_image , rotation_range=90, width_shift_range=0.3, height_shift_range=0.3, shear_range=0.5, zoom_range=0.3, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "mask_data_gen_args = dict( preprocessing_function = binarize_mask , rotation_range=90, width_shift_range=0.3, height_shift_range=0.3, shear_range=0.5, zoom_range=0.3, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "\n",
    "# img_data_gen_args = dict( preprocessing_function = normalize_image , rotation_range=90, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "# mask_data_gen_args = dict( preprocessing_function = binarize_mask , rotation_range=90, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "\n",
    "# img_data_gen_args = dict( preprocessing_function = normalize_image)\n",
    "# mask_data_gen_args = dict( preprocessing_function = binarize_mask )\n",
    "\n",
    "image_data_generator = ImageDataGenerator(**img_data_gen_args)\n",
    "mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_generator = image_data_generator.flow_from_directory(train_image_dir, \n",
    "                                                           seed=seed,target_size=(imgz_size, imgz_size), shuffle = True,\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           class_mode=None)  #Very important to set this otherwise it returns multiple numpy arrays \n",
    "                                                                            #thinking class mode is binary.\n",
    "\n",
    "\n",
    "mask_generator = mask_data_generator.flow_from_directory(train_mask_dir, \n",
    "                                                         seed=seed,target_size=(imgz_size, imgz_size), shuffle = True,\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         color_mode = 'grayscale',   #Read masks in grayscale\n",
    "                                                         class_mode=None)\n",
    "\n",
    "\n",
    "\n",
    "# valid_img_data_gen_args = dict( rescale = 1/255. , rotation_range=90, width_shift_range=0.3, height_shift_range=0.3, shear_range=0.5, zoom_range=0.3, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "# valid_mask_data_gen_args = dict( preprocessing_function = binarize_mask , rotation_range=90, width_shift_range=0.3, height_shift_range=0.3, shear_range=0.5, zoom_range=0.3, horizontal_flip=True, vertical_flip=True, fill_mode='reflect')\n",
    "\n",
    "# valid_img_data_gen_args = dict( preprocessing_function = normalize_image)\n",
    "# valid_mask_data_gen_args = dict( preprocessing_function = binarize_mask )\n",
    "\n",
    "# valid_image_data_generator = ImageDataGenerator(**valid_img_data_gen_args)\n",
    "# valid_mask_data_generator = ImageDataGenerator(**valid_mask_data_gen_args)\n",
    "\n",
    "\n",
    "# valid_img_generator = valid_image_data_generator.flow_from_directory(val_image_dir, \n",
    "#                                                                seed=seed,target_size=(imgz_size, imgz_size),\n",
    "#                                                                batch_size=batch_size, \n",
    "#                                                                class_mode=None) #Default batch size 32, if not specified here\n",
    "# valid_mask_generator = valid_mask_data_generator.flow_from_directory(val_mask_dir, \n",
    "#                                                                seed=seed,target_size=(imgz_size, imgz_size),\n",
    "#                                                                batch_size=batch_size, \n",
    "#                                                                color_mode = 'grayscale',   #Read masks in grayscale\n",
    "#                                                                class_mode=None)  #Default batch size 32, if not specified here\n",
    "\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "# val_generator = zip(valid_img_generator, valid_mask_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:25.284726Z",
     "start_time": "2024-02-06T08:48:24.774185Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:30.886273Z",
     "iopub.status.busy": "2024-02-06T08:35:30.885661Z",
     "iopub.status.idle": "2024-02-06T08:35:31.389779Z",
     "shell.execute_reply": "2024-02-06T08:35:31.388872Z"
    }
   },
   "outputs": [],
   "source": [
    "x = image_generator.next()\n",
    "y = mask_generator.next()\n",
    "for i in range(0,1):\n",
    "    image = x[i]\n",
    "    mask = y[i]\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:25.541327Z",
     "start_time": "2024-02-06T08:48:25.286220Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:31.397375Z",
     "iopub.status.busy": "2024-02-06T08:35:31.396950Z",
     "iopub.status.idle": "2024-02-06T08:35:31.613843Z",
     "shell.execute_reply": "2024-02-06T08:35:31.613137Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = valid_img_generator.next()\n",
    "# y = valid_mask_generator.next()\n",
    "# for i in range(0,1):\n",
    "#     image = x[i]\n",
    "#     mask = y[i]\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(mask)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:48:30.292365Z",
     "start_time": "2024-02-06T08:48:25.542495Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:31.621815Z",
     "iopub.status.busy": "2024-02-06T08:35:31.621607Z",
     "iopub.status.idle": "2024-02-06T08:35:36.349949Z",
     "shell.execute_reply": "2024-02-06T08:35:36.348260Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "\n",
    "\n",
    "layer_count_attn = 1\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Add\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2DTranspose, Concatenate, Input\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from swa.keras import SWA\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, Input\n",
    "\n",
    "\n",
    "initializer = keras.initializers.HeUniform(seed=None)\n",
    "\n",
    "\n",
    "def tf_avg_func(tensor):\n",
    "    x = tnp.array(tensor)\n",
    "    x = x[0] + x[1]+ x[2]+ x[3]+ x[4]+ x[5]\n",
    "    x = tnp.divide(x, 6)\n",
    "    return x\n",
    "\n",
    "def ASPP(inputs):\n",
    "    shape = inputs.shape\n",
    "\n",
    "    y_pool = AveragePooling2D(pool_size=(shape[1], shape[2]), name='average_pooling')(inputs)\n",
    "    y_pool = Conv2D(filters=256, kernel_size=1, padding='same', use_bias=False, kernel_initializer = initializer)(y_pool)\n",
    "    y_pool = BatchNormalization(name=f'bn_1')(y_pool)\n",
    "    y_pool = layers.LeakyReLU()(y_pool)\n",
    "    y_pool = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y_pool)\n",
    "\n",
    "    y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same', use_bias=False, kernel_initializer = initializer)(inputs)\n",
    "    y_1 = BatchNormalization()(y_1)\n",
    "    #y_1 = Activation('relu')(y_1)\n",
    "\n",
    "    y_6 = Conv2D(filters=256, kernel_size=3, dilation_rate=6, padding='same', use_bias=False, kernel_initializer = initializer)(inputs)\n",
    "    y_6 = BatchNormalization()(y_6)\n",
    "    #y_6 = Activation('relu')(y_6)\n",
    "\n",
    "    y_12 = Conv2D(filters=256, kernel_size=3, dilation_rate=12, padding='same', use_bias=False, kernel_initializer = initializer)(inputs)\n",
    "    y_12 = BatchNormalization()(y_12)\n",
    "    #y_12 = Activation('relu')(y_12)\n",
    "\n",
    "    y_18 = Conv2D(filters=256, kernel_size=3, dilation_rate=18, padding='same', use_bias=False, kernel_initializer = initializer)(inputs)\n",
    "    y_18 = BatchNormalization()(y_18)\n",
    "    #y_18 = Activation('relu')(y_18)\n",
    "\n",
    "    y = Concatenate()([y_pool, y_1, y_6, y_12, y_18])\n",
    "\n",
    "    y = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same', use_bias=False, kernel_initializer = initializer)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU()(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def SqueezeAndExcitation(inputs, ratio=8):\n",
    "    b, _, _, c = inputs.shape\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Dense(c//ratio, activation=\"relu\", use_bias=False)(x)\n",
    "    x = Dense(c, activation=\"sigmoid\", use_bias=False)(x)\n",
    "    \n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(inputs, out_ch, rate=1):\n",
    "    x = Conv2D(out_ch, 3, padding=\"same\", dilation_rate=rate, use_bias=False, kernel_initializer = initializer)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    #x = SqueezeAndExcitation(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block_simple(inputs, out_ch, rate=1):\n",
    "    x = Conv2D(out_ch, 3, padding=\"same\", dilation_rate=1  )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(out_ch, 3, padding=\"same\", dilation_rate=1 )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    shortcut = Conv2D(out_ch, 3, padding=\"same\", dilation_rate=1 )(inputs)\n",
    "    shortcut = layers.BatchNormalization(axis=3)(shortcut)\n",
    "    \n",
    "    res_path = layers.add([shortcut, x])\n",
    "    res_path = layers.Activation('relu')(res_path)\n",
    "\n",
    "    return res_path\n",
    "\n",
    "\n",
    "# Attention structure\n",
    "FILTER_NUM = 64 # number of basic filters for the first layer\n",
    "FILTER_SIZE = 3 # size of the convolutional filter\n",
    "UP_SAMP_SIZE = 2 # size of upsampling filters\n",
    "\n",
    "def repeat_elem(tensor, rep):\n",
    "\n",
    "    return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
    "                        arguments={'repnum': rep})(tensor)\n",
    "\n",
    "def gating_signal(input, out_size):\n",
    "\n",
    "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def attention_block(x, gating, inter_shape):\n",
    "    shape_x = K.int_shape(x)\n",
    "    shape_g = K.int_shape(gating)\n",
    "\n",
    "# Getting the x signal to the same shape as the gating signal\n",
    "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "# Getting the gating signal to the same number of filters as the inter_shape\n",
    "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(gating)\n",
    "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
    "                                strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
    "                                padding='same')(phi_g)  # 16\n",
    "\n",
    "    concat_xg = layers.add([upsample_g, theta_x])\n",
    "    act_xg = layers.Activation('relu')(concat_xg)\n",
    "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
    "\n",
    "    y = layers.multiply([upsample_psi, x])\n",
    "\n",
    "    global layer_count_attn\n",
    "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same' )(y)\n",
    "    \n",
    "    result_bn = layers.BatchNormalization(name='attention_'+str(layer_count_attn))(result)\n",
    "    layer_count_attn = layer_count_attn + 1\n",
    "    return result_bn\n",
    "\n",
    "\n",
    "def I_UNet_7(inputs, out_ch, int_ch, num_layers, rate=2):\n",
    "    \"\"\" Initial Conv \"\"\"\n",
    "    x = conv_block(inputs, out_ch) # (None, 256, 256, 64)\n",
    "    init_feats = x\n",
    "    \n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    skip = []\n",
    "    x = conv_block(x, int_ch) # (None, 256, 256, 32)    \n",
    "    skip.append(x)\n",
    "\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    x_128 = conv_block(x, int_ch) # (None, 128, 128, 32)\n",
    "    skip.append(x_128)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_128)\n",
    "    x_64 = conv_block(x, int_ch) # (None, 64, 64, 32)\n",
    "    skip.append(x_64)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_64)\n",
    "    x_32 = conv_block(x, int_ch) # (None, 32, 32, 32)\n",
    "    skip.append(x_32)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_32)\n",
    "    x_16 = conv_block(x, int_ch) # (None, 16, 16, 32)\n",
    "    skip.append(x_16)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_16)\n",
    "    x = conv_block(x, int_ch) # (None, 8, 8, 32)\n",
    "    skip.append(x)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    x = conv_block(x, int_ch, rate=rate) # (None, 8, 8, 32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "    skip.reverse()\n",
    "\n",
    "    x = Concatenate()([x, skip[0]]) # (None, 8, 8, 64)\n",
    "    x = conv_block(x, int_ch) # (None, 8, 8, 32)\n",
    "    \n",
    "    #-------------------------------------------------------------#\n",
    "    gating_16 = gating_signal(x, 8*FILTER_NUM)\n",
    "    att_16 = attention_block(x_16, gating_16, 8*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_16], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------#\n",
    "    gating_32 = gating_signal(x, 4*FILTER_NUM)\n",
    "    att_32 = attention_block(x_32, gating_32, 4*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_32], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "\n",
    "    #-------------------------------------------------------------#\n",
    "    gating_64 = gating_signal(x, 2*FILTER_NUM)\n",
    "    att_64 = attention_block(x_64, gating_64, 2*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_64], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------#\n",
    "    gating_128 = gating_signal(x, FILTER_NUM)\n",
    "    att_128 = attention_block(x_128, gating_128, FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_128], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = Concatenate()([x, skip[-1]])\n",
    "    x = conv_block(x, out_ch)\n",
    "\n",
    "    \"\"\" Add \"\"\"\n",
    "    x = Add()([x, init_feats])\n",
    "    return x\n",
    "\n",
    "\n",
    "def I_UNet_6(inputs, out_ch, int_ch, num_layers, rate=2):\n",
    "    \"\"\" Initial Conv \"\"\"\n",
    "    x = conv_block(inputs, out_ch) # (None, 128, 128, 128)\n",
    "    init_feats = x\n",
    "    \n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    skip = []\n",
    "    x = conv_block(x, int_ch) # (None, 128, 128, 32)\n",
    "    #print(x.shape)\n",
    "    skip.append(x)\n",
    "    \n",
    "\n",
    "\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    x_64 = conv_block(x, int_ch) # (None, 64, 64, 32)\n",
    "    #print(x_64.shape)\n",
    "    skip.append(x_64)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_64)\n",
    "    x_32 = conv_block(x, int_ch) # (None, 32, 32, 32)\n",
    "    #print(x_32.shape)\n",
    "    skip.append(x_32)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_32)\n",
    "    x_16 = conv_block(x, int_ch) # (None, 16, 16, 32)\n",
    "    #print(x_16.shape)\n",
    "    skip.append(x_16)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_16)\n",
    "    x = conv_block(x, int_ch) # (None, 8, 8, 32)\n",
    "    #print(x.shape)\n",
    "    skip.append(x)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    x = conv_block(x, int_ch, rate=rate)  # (None, 8, 8, 32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "    skip.reverse()\n",
    "\n",
    "    x = Concatenate()([x, skip[0]])  # (None, 8, 8, 64)\n",
    "    x = conv_block(x, int_ch)  # (None, 8, 8, 32)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------#\n",
    "    gating_16 = gating_signal(x, 8*FILTER_NUM)\n",
    "    att_16 = attention_block(x_16, gating_16, 8*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_16], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------#\n",
    "    gating_32 = gating_signal(x, 4*FILTER_NUM)\n",
    "    att_32 = attention_block(x_32, gating_32, 4*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_32], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "\n",
    "    #-------------------------------------------------------------#\n",
    "    gating_64 = gating_signal(x, 2*FILTER_NUM)\n",
    "    att_64 = attention_block(x_64, gating_64, 2*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_64], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)  # (None, 128, 128, 32)\n",
    "    x = Concatenate()([x, skip[-1]])\n",
    "    x = conv_block(x, out_ch)  # (None, 128, 128, 128)\n",
    "\n",
    "    \"\"\" Add \"\"\"\n",
    "    x = Add()([x, init_feats])  # (None, 128, 128, 128)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def I_UNet_5(inputs, out_ch, int_ch, num_layers, rate=2):\n",
    "    \"\"\" Initial Conv \"\"\"\n",
    "    x = conv_block(inputs, out_ch) # (None, 128, 128, 128)\n",
    "    init_feats = x\n",
    "    \n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    skip = []\n",
    "    x = conv_block(x, int_ch) # (None, 128, 128, 32)\n",
    "    skip.append(x)\n",
    "\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    x_32 = conv_block(x, int_ch) # (None, 32, 32, 32)\n",
    "    skip.append(x_32)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_32)\n",
    "    x_16 = conv_block(x, int_ch) # (None, 16, 16, 32)\n",
    "    skip.append(x_16)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_16)\n",
    "    x = conv_block(x, int_ch) # (None, 8, 8, 32)\n",
    "    skip.append(x)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    x = conv_block(x, int_ch, rate=rate)  # (None, 8, 8, 32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "    skip.reverse()\n",
    "\n",
    "    x = Concatenate()([x, skip[0]])  # (None, 8, 8, 64)\n",
    "    x = conv_block(x, int_ch)  # (None, 8, 8, 32)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------#\n",
    "    gating_16 = gating_signal(x, 8*FILTER_NUM)\n",
    "    att_16 = attention_block(x_16, gating_16, 8*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_16], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------#\n",
    "    gating_32 = gating_signal(x, 4*FILTER_NUM)\n",
    "    att_32 = attention_block(x_32, gating_32, 4*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_32], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)  # (None, 128, 128, 32)\n",
    "    x = Concatenate()([x, skip[-1]])\n",
    "    x = conv_block(x, out_ch)  # (None, 128, 128, 128)\n",
    "\n",
    "    \"\"\" Add \"\"\"\n",
    "    x = Add()([x, init_feats])  # (None, 128, 128, 128)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def I_UNet_4(inputs, out_ch, int_ch, num_layers, rate=2):\n",
    "    \"\"\" Initial Conv \"\"\"\n",
    "    x = conv_block(inputs, out_ch) # (None, 128, 128, 128)\n",
    "    init_feats = x\n",
    "    \n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    skip = []\n",
    "    x = conv_block(x, int_ch) # (None, 128, 128, 32)\n",
    "    skip.append(x)\n",
    "\n",
    "\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    x_16 = conv_block(x, int_ch) # (None, 16, 16, 32)\n",
    "    skip.append(x_16)\n",
    "    \n",
    "    x = MaxPool2D((2, 2))(x_16)\n",
    "    x = conv_block(x, int_ch) # (None, 8, 8, 32)\n",
    "    skip.append(x)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    x = conv_block(x, int_ch, rate=rate)  # (None, 8, 8, 32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "    skip.reverse()\n",
    "\n",
    "    x = Concatenate()([x, skip[0]])  # (None, 8, 8, 64)\n",
    "    x = conv_block(x, int_ch)  # (None, 8, 8, 32)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------#\n",
    "    gating_16 = gating_signal(x, 8*FILTER_NUM)\n",
    "    att_16 = attention_block(x_16, gating_16, 8*FILTER_NUM)\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n",
    "    x = layers.concatenate([x, att_16], axis=3)\n",
    "    x = conv_block(x, int_ch)\n",
    "    #-------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)  # (None, 128, 128, 32)\n",
    "    x = Concatenate()([x, skip[-1]])\n",
    "    x = conv_block(x, out_ch)  # (None, 128, 128, 128)\n",
    "\n",
    "    \"\"\" Add \"\"\"\n",
    "    x = Add()([x, init_feats])  # (None, 128, 128, 128)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def bridge_block(inputs, out_ch, int_ch):\n",
    "    \"\"\" Initial Conv \"\"\"\n",
    "    x0 = conv_block(inputs, out_ch, rate=1)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    x1 = conv_block(x0, int_ch, rate=1)\n",
    "    x2 = conv_block(x1, int_ch, rate=2)\n",
    "    x3 = conv_block(x2, int_ch, rate=4)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    x4 = conv_block(x3, int_ch, rate=8)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    x = Concatenate()([x4, x3])\n",
    "    x = conv_block(x, int_ch, rate=4)\n",
    "\n",
    "    x = Concatenate()([x, x2])\n",
    "    x = conv_block(x, int_ch, rate=2)\n",
    "\n",
    "    x = Concatenate()([x, x1])\n",
    "    x = conv_block(x, out_ch, rate=1)\n",
    "\n",
    "    \"\"\" Addition \"\"\"\n",
    "    x = Add()([x, x0])\n",
    "    return x\n",
    "\n",
    "def histosegplusplus(input_shape, out_ch, int_ch, num_classes=2):\n",
    "    \"\"\" Input Layer \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    s0 = inputs\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    skip = []\n",
    "    \n",
    "    s1 = I_UNet_7(s0, out_ch[0], int_ch[0], 7)\n",
    "    s1 = conv_block(s1, out_ch[0])\n",
    "    s1 = SqueezeAndExcitation(s1)\n",
    "    skip.append(s1)\n",
    "    p1 = MaxPool2D((2, 2))(s1)\n",
    "\n",
    "    s2 = I_UNet_6(p1, out_ch[1], int_ch[1], 6)\n",
    "    s2 = conv_block(s2, out_ch[1])\n",
    "    s2 = SqueezeAndExcitation(s2)\n",
    "    skip.append(s2)\n",
    "    p2 = MaxPool2D((2, 2))(s2)\n",
    "\n",
    "    s3 = I_UNet_5(p2, out_ch[2], int_ch[2], 5)\n",
    "    s3 = conv_block(s3, out_ch[2])\n",
    "    s3 = SqueezeAndExcitation(s3)\n",
    "    skip.append(s3)\n",
    "    p3 = MaxPool2D((2, 2))(s3)\n",
    "\n",
    "    s4 = I_UNet_4(p3, out_ch[3], int_ch[3], 4)\n",
    "    s4 = conv_block(s4, out_ch[3])\n",
    "    s4 = SqueezeAndExcitation(s4) \n",
    "    skip.append(s4)\n",
    "    p4 = MaxPool2D((2, 2))(s4)\n",
    "\n",
    "    s5 = bridge_block(p4, out_ch[4], int_ch[4])\n",
    "    s5 = conv_block(s5, out_ch[4])\n",
    "    s5 = SqueezeAndExcitation(s5)\n",
    "    skip.append(s5)\n",
    "    p5 = MaxPool2D((2, 2))(s5)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = bridge_block(p5, out_ch[5], int_ch[5])\n",
    "    b1 = ASPP(b1)\n",
    "    b2 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(b1)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    skip.reverse()\n",
    "    \n",
    "    d1 = Concatenate()([b2, s5])\n",
    "    d1 = bridge_block(d1, out_ch[6], int_ch[6])\n",
    "\n",
    "\n",
    "    \n",
    "    gating = gating_signal(d1, 16*FILTER_NUM)\n",
    "    att = attention_block(s4, gating, 8*FILTER_NUM)\n",
    "    u1 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d1)\n",
    "    d2 = Concatenate()([u1, att])\n",
    "    d2 = I_UNet_4(d2, out_ch[7], int_ch[7], 4)\n",
    "\n",
    "    \n",
    "    \n",
    "    gating = gating_signal(d2, 8*FILTER_NUM)\n",
    "    att = attention_block(s3, gating, 4*FILTER_NUM)\n",
    "    u2 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d2)\n",
    "    d3 = Concatenate()([u2, att])\n",
    "    d3 = I_UNet_5(d3, out_ch[8], int_ch[8], 5)\n",
    "\n",
    "    \n",
    "    \n",
    "    gating = gating_signal(d3, 4*FILTER_NUM)\n",
    "    att = attention_block(s2, gating, 2*FILTER_NUM)\n",
    "    u3 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d3)\n",
    "    d4 = Concatenate()([u3, att])\n",
    "    d4 = I_UNet_6(d4, out_ch[9], int_ch[9], 6)\n",
    "\n",
    "    \n",
    "    \n",
    "    gating = gating_signal(d4, 2*FILTER_NUM)\n",
    "    att = attention_block(s1, gating, 2*FILTER_NUM)\n",
    "    u4 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d4)\n",
    "    d5 = Concatenate()([u4, att])\n",
    "    d5 = I_UNet_7(d5, out_ch[10], int_ch[10], 7)\n",
    "\n",
    "    \n",
    "\n",
    "    \"\"\" Side Outputs \"\"\"\n",
    "\n",
    "    z1 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(d5)\n",
    "\n",
    "    z2 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(d4)\n",
    "    z2 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(z2)\n",
    "\n",
    "    z3 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(d3)\n",
    "    z3 = UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(z3)\n",
    "\n",
    "    z4 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(d2)\n",
    "    z4 = UpSampling2D(size=(8, 8), interpolation=\"bilinear\")(z4)\n",
    "\n",
    "    z5 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(d1)\n",
    "    z5 = UpSampling2D(size=(16, 16), interpolation=\"bilinear\")(z5)\n",
    "\n",
    "    z6 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(b1)\n",
    "    z6 = UpSampling2D(size=(32, 32), interpolation=\"bilinear\")(z6)\n",
    "    \n",
    "    \n",
    "    \n",
    "    can_1 = Concatenate()([z1, z2, z3, z4, z5, z6])    \n",
    "    \n",
    "    o_5 = Conv2D(num_classes, 3, padding=\"same\", use_bias=False, kernel_initializer = initializer)(can_1)\n",
    "    \n",
    "    o_5 = Activation(\"sigmoid\")(o_5)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs=[o_5])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model(input_shape, num_classes=1):\n",
    "    out_ch = [64, 128, 256, 512, 512, 512, 512, 256, 128, 64, 64]\n",
    "    int_ch = [32, 32, 64, 128, 256, 256, 256, 128, 64, 32, 16]\n",
    "    model = histosegplusplus(input_shape, out_ch, int_ch, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_hist():\n",
    "    return build_model((256, 256, 3))\n",
    "    \n",
    "model = build_model_hist()\n",
    "\n",
    "\n",
    "model.summary(expand_nested=True, show_trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:36.489290Z",
     "iopub.status.busy": "2024-02-06T08:35:36.488919Z",
     "iopub.status.idle": "2024-02-06T08:35:41.058845Z",
     "shell.execute_reply": "2024-02-06T08:35:41.057866Z"
    }
   },
   "outputs": [],
   "source": [
    "print_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:41.077322Z",
     "iopub.status.busy": "2024-02-06T08:35:41.076973Z",
     "iopub.status.idle": "2024-02-06T08:35:41.082123Z",
     "shell.execute_reply": "2024-02-06T08:35:41.081237Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss(per_image = True) \n",
    "focal_loss = sm.losses.CategoricalFocalLoss(gamma=2.)\n",
    "jaccard_loss = sm.losses.JaccardLoss()\n",
    "binary_focal_loss = sm.losses.BinaryFocalLoss(gamma=2.)\n",
    "binary_crossentropy = sm.losses.BinaryCELoss()\n",
    "\n",
    "total_loss = (0.4*binary_crossentropy) + (0.6*binary_focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:41.086640Z",
     "iopub.status.busy": "2024-02-06T08:35:41.086258Z",
     "iopub.status.idle": "2024-02-06T08:35:41.096666Z",
     "shell.execute_reply": "2024-02-06T08:35:41.095681Z"
    }
   },
   "outputs": [],
   "source": [
    "keras_iou = tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=0.5)\n",
    "iou_score = sm.metrics.IOUScore(per_image = True, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_aware_loss(y_true, y_pred, edge_weight=2.0):\n",
    "    \"\"\"\n",
    "    Edge-aware loss function for segmentation tasks.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Ground truth masks.\n",
    "    - y_pred: Predicted masks.\n",
    "    - edge_weight: Weight multiplier for edge regions.\n",
    "    \n",
    "    Returns:\n",
    "    - Loss value.\n",
    "    \"\"\"\n",
    "    # Calculate binary cross-entropy\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    bce = tf.expand_dims(bce, axis=-1)  # Ensure BCE has the correct shape\n",
    "    \n",
    "    # Detect edges in the ground truth mask\n",
    "    sobel_x = tf.image.sobel_edges(y_true)[:,:,:,:,0]\n",
    "    sobel_y = tf.image.sobel_edges(y_true)[:,:,:,:,1]\n",
    "    edge_mask = tf.sqrt(tf.square(sobel_x) + tf.square(sobel_y))\n",
    "    edge_mask = tf.cast(edge_mask > 0.1, tf.float32)  # Threshold for edges\n",
    "    \n",
    "    # Ensure edge_mask is compatible for broadcasting\n",
    "    edge_mask = tf.reduce_mean(edge_mask, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Weights for edge regions\n",
    "    weights = 1 + edge_mask * (edge_weight - 1)\n",
    "    \n",
    "    # Apply weights\n",
    "    weighted_bce = tf.multiply(bce, weights)\n",
    "    \n",
    "    return tf.reduce_mean(weighted_bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:41.101501Z",
     "iopub.status.busy": "2024-02-06T08:35:41.100998Z",
     "iopub.status.idle": "2024-02-06T08:35:41.131454Z",
     "shell.execute_reply": "2024-02-06T08:35:41.130641Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "BCE = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def model_compile(model_x):\n",
    "    model_x.compile(optimizer=opt, loss = edge_aware_loss, metrics= [iou_score])\n",
    "\n",
    "model_compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:41.135613Z",
     "iopub.status.busy": "2024-02-06T08:35:41.135317Z",
     "iopub.status.idle": "2024-02-06T08:35:41.140232Z",
     "shell.execute_reply": "2024-02-06T08:35:41.139449Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class TimeHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_start_time)\n",
    "        print(f\"Epoch {epoch+1} took {format_time(self.times[-1])}\")\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:41.144529Z",
     "iopub.status.busy": "2024-02-06T08:35:41.144225Z",
     "iopub.status.idle": "2024-02-06T08:35:41.152925Z",
     "shell.execute_reply": "2024-02-06T08:35:41.152062Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath= path_1 + \"{epoch:04d}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_best_only=False, save_weights_only=True, verbose= 1)\n",
    "\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(path_2 + \"logs.csv\", separator=\",\", append=False)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.000000001, verbose=1)\n",
    "\n",
    "\n",
    "num_train_imgs = len(os.listdir(train_image_dir + 'train/'))\n",
    "steps_per_epoch = num_train_imgs //batch_size\n",
    "print(\"num_train_imgs -------------- = \" + str(num_train_imgs))\n",
    "print(\"steps_per_epoch -------------- = \" + str(steps_per_epoch))\n",
    "\n",
    "\n",
    "start_epoch = 100\n",
    "\n",
    "from swa.keras import SWA\n",
    "swa = SWA(start_epoch=start_epoch, lr_schedule='cyclic', swa_lr=0.0001, swa_lr2=0.01, swa_freq=3, batch_size=batch_size, verbose=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", restore_best_weights=True, patience=3, verbose=1)\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "schedule_lr = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, time_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:35:41.164882Z",
     "iopub.status.busy": "2024-02-06T08:35:41.164478Z",
     "iopub.status.idle": "2024-02-06T08:36:56.009330Z",
     "shell.execute_reply": "2024-02-06T08:36:56.008405Z"
    }
   },
   "outputs": [],
   "source": [
    "if (training):\n",
    "    start_time = time.time()\n",
    "\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        epochs=num_epochs, \n",
    "                        callbacks=callbacks_list,\n",
    "                        batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_training_time = end_time - start_time\n",
    "    total_training_time_formatted = format_time(total_training_time)\n",
    "\n",
    "    print(f\"Total training time: {total_training_time:.3f} seconds\")\n",
    "    print(f\"Total training time: {total_training_time_formatted}\")\n",
    "\n",
    "    model.save_weights(path_1 + 'last_weights.hdf5')\n",
    "    \n",
    "    epoch_times = time_callback.times\n",
    "    calc_time(epoch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####---------------------------------------------------------Evaluation Evaluation Evaluation Evaluation Evaluation Evaluation Evaluation Evaluation Evaluation Evaluation Evaluation---------------------------------------------------------###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:36:56.016951Z",
     "iopub.status.busy": "2024-02-06T08:36:56.016469Z",
     "iopub.status.idle": "2024-02-06T08:36:56.071804Z",
     "shell.execute_reply": "2024-02-06T08:36:56.070999Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images_arg = \"\"\n",
    "test_masks_arg = \"\"\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.load(test_images_arg)\n",
    "y_test = np.load(test_masks_arg)\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "X_test = aug(image=X_test)['image']\n",
    "print('Min: %.3f, Max: %.3f' % (X_test.min(), X_test.max()))\n",
    "# print(np.unique(X_test))\n",
    "\n",
    "y_test = np.where(y_test>0, 1, 0)\n",
    "y_test = y_test.astype(np.float32())\n",
    "print('Min: %.3f, Max: %.3f' % (y_test.min(), y_test.max()))\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:36:56.076190Z",
     "iopub.status.busy": "2024-02-06T08:36:56.075845Z",
     "iopub.status.idle": "2024-02-06T08:36:56.253725Z",
     "shell.execute_reply": "2024-02-06T08:36:56.252971Z"
    }
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, X_test.shape[0]-1)\n",
    "print(i)\n",
    "image = X_test[i]\n",
    "mask = y_test[i]\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:36:56.259281Z",
     "iopub.status.busy": "2024-02-06T08:36:56.259093Z",
     "iopub.status.idle": "2024-02-06T08:36:56.263174Z",
     "shell.execute_reply": "2024-02-06T08:36:56.262473Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_clear_and_build():\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = 0\n",
    "    model = build_model_hist()\n",
    "    model_compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:36:56.267054Z",
     "iopub.status.busy": "2024-02-06T08:36:56.266709Z",
     "iopub.status.idle": "2024-02-06T08:37:03.690610Z",
     "shell.execute_reply": "2024-02-06T08:37:03.689748Z"
    }
   },
   "outputs": [],
   "source": [
    "model_clear_and_build()\n",
    "\n",
    "pred_index = -1\n",
    "df_combined = 0\n",
    "df_counter = 0\n",
    "\n",
    "current_cwd = os.getcwd()\n",
    "weights_folder_path = current_cwd + \"/results/\"\n",
    "dirs_ = sorted( os.listdir(weights_folder_path) )\n",
    "\n",
    "try:\n",
    "    dirs_.remove('last_weights.hdf5')\n",
    "    dirs_.remove('last_weights_2.hdf5')\n",
    "    dirs_.remove('last_weights_3.hdf5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "pred = model.predict(X_test[0:3], batch_size=1, verbose=0)\n",
    "\n",
    "if isinstance(pred, list):\n",
    "    len_pred = len(pred)\n",
    "else:\n",
    "    len_pred = 1\n",
    "\n",
    "print(\"No of Prediction Activations: \", len_pred)\n",
    "\n",
    "if len_pred == X_test.shape[0]:\n",
    "    len_pred = 1\n",
    "    pred_index = -1\n",
    "elif (len_pred == 1):\n",
    "    len_pred = 1\n",
    "    pred_index = -1\n",
    "else:\n",
    "    pred_index = 0\n",
    "\n",
    "\n",
    "print(\"Pred_Index: \", pred_index)\n",
    "\n",
    "\n",
    "\n",
    "#### Controling Predictions\n",
    "\n",
    "# 0 = all predictions\n",
    "# 1 = specific prediction\n",
    "# 2 = default\n",
    "\n",
    "\n",
    "control_sup = 2\n",
    "\n",
    "if (control_sup == 1):\n",
    "\n",
    "    ##################\n",
    "    len_pred = 1\n",
    "    pred_index = 0\n",
    "    print(\"No of Prediction Activations: \", len_pred)\n",
    "    print(\"Pred_Index: \", pred_index)\n",
    "    ##################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:37:03.694523Z",
     "iopub.status.busy": "2024-02-06T08:37:03.694158Z",
     "iopub.status.idle": "2024-02-06T08:37:31.374056Z",
     "shell.execute_reply": "2024-02-06T08:37:31.373188Z"
    }
   },
   "outputs": [],
   "source": [
    "if (training):\n",
    "    def df_convert(df, ep_str, index_str):\n",
    "        df = df.transpose()\n",
    "        #df = df[1:]\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.insert(0, 'Index_Out', index_str)\n",
    "        df.insert(0, 'Epoch', ep_str)\n",
    "        return df\n",
    "\n",
    "    for f in range(0, len(dirs_)):\n",
    "        pbar = tqdm( range(0, len_pred) )\n",
    "        epoch_f = dirs_[f]\n",
    "\n",
    "        for p in pbar:\n",
    "            \n",
    "            model_clear_and_build()\n",
    "            \n",
    "            pbar.set_description(\"Epoch: \" + epoch_f)\n",
    "            model.load_weights(weights_folder_path + dirs_[f])\n",
    "            \n",
    "            pred_sm_f = model.predict(X_test, verbose=0, batch_size=1)\n",
    "    \n",
    "            if (control_sup == 0):\n",
    "                pred_index = p\n",
    "                pred_sm_f = pred_sm_f[pred_index]\n",
    "            \n",
    "            if (control_sup == 1):\n",
    "                pred_index = pred_index\n",
    "                pred_sm_f = pred_sm_f[pred_index]\n",
    "            \n",
    "\n",
    "            \n",
    "            df = eval_all_metrics(y_test, pred_sm_f)\n",
    "            df = df.round(4)\n",
    "            \n",
    "            df = df_convert(df, epoch_f, pred_index)\n",
    "            \n",
    "            if df_counter == 0:\n",
    "                df_combined = df\n",
    "                \n",
    "                df_counter = 1\n",
    "            else:\n",
    "                df_combined = pd.concat([df_combined, df], ignore_index=True)\n",
    "            \n",
    "            df_combined.to_csv(eval_results_path +\"Epoch_\"+ epoch_f +\"_eval_all_metrics.csv\")\n",
    "                \n",
    "        \n",
    "            \n",
    "    df_combined.to_csv(eval_results_path + \"all_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:37:31.378459Z",
     "iopub.status.busy": "2024-02-06T08:37:31.378027Z",
     "iopub.status.idle": "2024-02-06T08:37:31.400278Z",
     "shell.execute_reply": "2024-02-06T08:37:31.399441Z"
    }
   },
   "outputs": [],
   "source": [
    "if (training):\n",
    "    df = pd.read_csv(eval_results_path + \"all_eval.csv\")\n",
    "\n",
    "    # Find the rows with maximum values in \"sm_iou_score_per_image\" grouped by \"Epoch\" and \"Index_Out\"\n",
    "    max_rows = df.loc[df.groupby([ \"Index_Out\"])[\"sm_iou_score_per_image\"].idxmax()]\n",
    "    max_rows = max_rows.drop(columns=['Unnamed: 0'])\n",
    "    max_rows.to_csv(eval_results_path + \"all_eval_max_filter.csv\")\n",
    "    show(max_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:37:31.404658Z",
     "iopub.status.busy": "2024-02-06T08:37:31.404239Z",
     "iopub.status.idle": "2024-02-06T08:37:31.413231Z",
     "shell.execute_reply": "2024-02-06T08:37:31.412403Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(eval_results_path + \"all_eval_max_filter.csv\")\n",
    "max_iou_index = df['sm_iou_score_per_image'].idxmax()\n",
    "max_iou_row = df.iloc[max_iou_index]\n",
    "max_iou_weight_file = weights_folder_path + max_iou_row[1]\n",
    "best_epoch_str = max_iou_row[1]\n",
    "print(max_iou_weight_file)\n",
    "print(best_epoch_str)\n",
    "pred_index = df.iloc[max_iou_index][2]\n",
    "print(pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:37:31.417710Z",
     "iopub.status.busy": "2024-02-06T08:37:31.417300Z",
     "iopub.status.idle": "2024-02-06T08:37:31.421278Z",
     "shell.execute_reply": "2024-02-06T08:37:31.420497Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T08:37:31.425724Z",
     "iopub.status.busy": "2024-02-06T08:37:31.425294Z",
     "iopub.status.idle": "2024-02-06T08:37:35.223543Z",
     "shell.execute_reply": "2024-02-06T08:37:35.222918Z"
    }
   },
   "outputs": [],
   "source": [
    "model_clear_and_build()\n",
    "model.load_weights(max_iou_weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_full_test_directory = \"\"\n",
    "mask_full_test_directory = \"\"\n",
    "\n",
    "create_pred_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_img_size = 256\n",
    "patch_step_size = 128\n",
    "\n",
    "resize_img = True\n",
    "resize_wh = [1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir_prefix = \"predictions/pred_tta/\"\n",
    "tta = True\n",
    "pred_full(tta, control_sup, pred_index, pred_dir_prefix, model, image_full_test_directory, mask_full_test_directory, patch_img_size, patch_step_size, resize_img, resize_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pred = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.load(\"predictions/pred_tta/preds_array.npy\")\n",
    "pred = np.load(\"predictions/pred_tta/y_tests_array.npy\")\n",
    "\n",
    "\n",
    "print('Min: %.3f, Max: %.3f' % (y_test.min(), y_test.max()))\n",
    "print(np.unique(y_test))\n",
    "\n",
    "print('Min: %.3f, Max: %.3f' % (pred.min(), pred.max()))\n",
    "print(np.unique(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_all_metrics(y_test, pred, all = 1, threshold_pred = threshold_pred)\n",
    "\n",
    "if (training):\n",
    "    df.to_csv(eval_results_path + \"all_metrics_\" + best_epoch_str + \".csv\")\n",
    "else:\n",
    "    df.to_csv(eval_results_path + \"all_metrics_manual_testing_\" + best_epoch_str + \".csv\")\n",
    "\n",
    "df = df.round(4)\n",
    "style_df = highlight_values_with_colors(df, 'Score')\n",
    "style_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
